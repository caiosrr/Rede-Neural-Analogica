\documentclass[12pt, a4paper]{article}

% --- Pacotes Fundamentais ---
\usepackage[utf8]{inputenc}     % Codificação do arquivo
\usepackage[T1]{fontenc}        % Seleção de códigos de fonte
\usepackage[brazil]{babel}      % Idioma para hifenização e tabelas
\usepackage{indentfirst}        % Indenta o primeiro parágrafo de cada seção
\usepackage{graphicx}           % Inclusão de gráficos
\usepackage{amsmath, amssymb}   % Pacotes matemáticos (essencial para o perceptron)
\usepackage{geometry}           % Configuração de margens
%\usepackage{setspace}           % Espaçamento entre linhas
\usepackage{float}
%\usepackage{url}
%\usepackage{hyperref} % Se estiver usando links
%\usepackage[capitalise, nameinlink]{cleveref}
%\usepackage[american]{circuitikz}
%\usepackage{tikz}
%\usetikzlibrary{fit}
%\usepackage{subcaption}
\usepackage{booktabs}
%\definecolor{verdeescuro}{rgb}{0.0, 0.5, 0.0}

% --- Configuração das Margens (Padrão ABNT) ---
\geometry{a4paper, left=3cm, top=3cm, right=2cm, bottom=2cm}

% --- Início do Documento ---
\begin{document}

% --- Capa ---
\begin{titlepage}
    \begin{center}
        \begin{center}
            % Nome da Universidade maior
            \textbf{\Large UNIVERSIDADE DE SÃO PAULO}\\[0.2cm] 
            
            % Nome do Instituto um pouco menor
            \textbf{\large INSTITUTO DE FÍSICA}\\[0.2cm]
            
            % Sigla (Opcional, mas se quiser manter)
            \textbf{\small (IFUSP)}
        \end{center}
        \vspace{3.5cm}

        % Informações da Disciplina
        \begin{flushleft}
            \textbf{Disciplina:} Física Experimental VI - 4302314\\
            \textbf{Professor(a):} Germano Maioli Penello
        \end{flushleft}

        \vspace{3.0cm}

        % Título do Projeto
        {\Large \textbf{PERCEPTRON ANALÓGICO}}

        \vspace{3.5cm}

        % Integrantes e N° USP (Alinhados à direita)
        \begin{flushright}
            \begin{minipage}{0.6\textwidth} % Cria uma caixa para alinhar os nomes
                \textbf{Integrantes do Grupo:}\\[0.5cm]
                Nome: Caio de Sousa Ribeiro \dotfill N° USP: 13687071\\
                Nome: Filipe Santos Costa \dotfill N° USP: 13734271\\
                Ricardo D’avila Biasotto Mano:  \dotfill N° USP: 13685937\\
            \end{minipage}
        \end{flushright}

        \vfill % Empurra a data para o final da página

        % Local e Data
        \textbf{São Paulo}\\
        \textbf{\today}
    \end{center}
\end{titlepage}

% --- Resumo ---
\begin{abstract}
    \noindent
    Este relatório apresenta o desenvolvimento de um neurônio artificial do tipo Perceptron utilizando componentes eletrônicos analógicos. O objetivo principal foi implementar a soma ponderada de entradas e uma função de ativação não-linear utilizando Amplificadores Operacionais. Os estudos sobre funcionamento de redes neurais e o auxílio de aplicativos de simulação permitiram o desenvolvimento do circuito físico em conjunto com códigos de programação, performando similarmente ao que é esperado de uma rede neural simples.

    \vspace{\baselineskip}
    \noindent
    \textbf{Palavras-chave:} Perceptron. Amplificador Operacional. Redes Neurais.
\end{abstract}

% --- Sumário (Opcional) ---
\tableofcontents
\newpage

% --- Corpo do Texto ---
% \onehalfspacing % Espaçamento 1.5

\section{Introdução}
As redes neurais artificiais são modelos computacionais baseados no sistema nervoso de organismos inteligentes, capazes de reconhecer padrões complexos por meio do processamento de dados. Essa estrutura é a base para o chamado aprendizado de máquina, amplamente debatido na atualidade devido à popularização das Inteligências Artificiais (IAs).

É possível descrever uma rede neural como um conjunto de neurônios interconectados, distribuídos em camadas, capazes de processar valores de entrada e associá-los a uma resposta de saída. O processamento é feito através de operações que atribuem "pesos" a determinados valores e, por fim, retorna um resultado específico, o qual pode ser a resposta final ou então aplicado a uma nova etapa de processamento.

Para um mesmo tipo de neurônio, a estrutura das operações matemáticas permanece fixa. Ou seja, a função utilizada para o processamento continua a mesma, independentemente dos valores atribuídos aos pesos. Dessa forma, ao comparar os resultados obtidos com os esperados, o sistema deve ser capaz de ajustar esses pesos para garantir a resposta correta a uma determinada tarefa. Esse processo iterativo de ajuste é o que caracteriza o aprendizado da rede neural.

\begin{figure}[H]
    \centering
    % \includegraphics[width=0.5\linewidth]{Imagens/Neural_network.jpg}
    \caption{Esquema de rede neural em que neurônios interconectados e distribuídos em camadas processam valores e apresentam uma resultado na saída.}
    \label{fig:neural_network}
\end{figure}

Embora as redes neurais artificiais sejam, em sua maioria, implementadas em softwares e executadas a partir de processamentos digitais, existe uma outra vertente focada no nível de hardware. Nesses casos, as abstrações matemáticas dão lugar a componentes físicos que manipulam grandezas de forma a simular as operações necessárias para o processamento.

Este trabalho tem, portanto, o objetivo de implementar um neurônio artificial em um circuito eletrônico. O circuito deverá apresentar componentes capazes de cumprir atribuições análogas àquelas descritas pela teoria das redes neurais, como por exemplo: o uso de chaves para estabelecer valores de entradas ($0 \rightarrow \text{chave aberta, }1 \rightarrow \text{chave fechada}$); o uso de potenciômetros que representam os peso do modelo e, por fim, o auxílio de Amplificadores Operacionais que exercem tarefas como soma algébrica de sinais e comparador de limiar, que são essenciais para o processamento das informações.


\section{Fundamentação Teórica}
\subsection{O Neurônio Artificial}
O modelo computacional mais simples de um neurônio artificial é o chamado perceptron, cuja base foi elaborada pela primeira vez em 1943 pelo neurofisiologista Warren McCulloch e pelo matemático Walter Pitts \cite{mcculloch_pitts_1943}. Projetado para processar sinais de entrada e exibir um valor de saída, o perceptron é capaz de associar um peso a cada entrada, que indicará a influência do respectivo sinal na operação. Após ser realizada uma soma dos sinais de entrada ponderados pelos pesos associados, o resultado é, por fim, comparado a um limiar ($b$), que definirá o valor de saída do neurônio.

De modo a exemplificar, seja $\vec{x} \in \mathbb{R}^n$ um vetor de valores de entrada, e $\vec{w} \in \mathbb{R}^n$ o vetor de pesos associados àqueles valores. Podemos definir a soma ponderada como:
$$S(\vec{w},\vec{x}) = \vec{w}\cdot\vec{x} = \sum_{i=1}^n w_i x_i$$
Sendo $b$ o limiar, é possível definir a resposta do neurônio a partir da função "degrau":
\begin{equation*}
    f(\vec{w},\vec{x}) = \begin{cases}
        1 & \text{se } \vec{w}\cdot\vec{x} > b \\
        0 & \text{se } \vec{w}\cdot\vec{x} \leq b
    \end{cases}
\end{equation*}

Portanto, o perceptron deve ser capaz de gerar uma resposta binária a partir das entradas e pesos associados. A Fig. \ref{fig:mccul} demonstra visualmente essa interação.

\begin{figure}[H]
    \centering
    % \includegraphics[width=0.7\linewidth]{Imagens/mccul.jpeg}
    \caption{Operação de soma ponderada das entradas, como proposto por McCulloch e Pitts.}
    \label{fig:mccul}
\end{figure}

Para que o neurônio retorne os valores esperados e execute determinada função, é necessário incluir uma etapa de aprendizado. Nesse sentido, o perceptron deve ser capaz de ajustar os pesos das entradas de maneira a reduzir o erro quadrático entre a resposta obtida e a resposta esperada. Na próxima seção será detalhado o método utilizado com o intuito de gerar o aprendizado.


\subsection{O Algoritmo Backpropagation}
 O algoritmo de \textit{Backpropagation} fundamenta o treinamento de redes neurais através da regra da cadeia. O processo consiste em calcular a sensibilidade do erro total em relação a cada peso, termo denotado por $\dfrac{\partial E}{\partial w_i}$. Com base nesse valor, os pesos são ajustados para minimizar o erro. O nome "retropropagação" deve-se ao fluxo do cálculo, que ocorre da camada de saída para a camada de entrada, evitando redundâncias computacionais.

 Uma vez obtido o gradiente, utiliza-se um algoritmo de otimização, como por exemplo método de Descida do Gradiente (\textit{Gradient Descent}), para ajustar os parâmetros da rede. A regra de atualização é dada por:
\begin{equation}
    w_i^{(t+1)} = w_i^{(t)} - \eta \cdot \dfrac{\partial E}{\partial w_i}
\end{equation}
onde $\eta$ representa a taxa de aprendizado (\textit{learning rate}), um parâmetro escalar que define a magnitude do ajuste a cada iteração.

A escolha adequada de $\eta$ é crucial: valores excessivamente altos podem causar oscilações e divergência no erro, enquanto valores muito baixos resultam em uma convergência lenta em direção ao mínimo global da função de custo (erro).
\section{Projeto e Modelagem do circuito}
\subsection{Materiais e Componentes}

\begin{table}[H]
    \centering
    \caption{Lista de materiais e componentes utilizados na construção dos neurônios.}
    \label{tab:componentes_neuronios}
    \begin{tabular}{lccc}
        \toprule
        \textbf{Componente} & \textbf{Valor/Tipo} & \textbf{Neurônio Entrada}& \textbf{Neurônio Saída} \\
        \midrule
        \multicolumn{4}{l}{\textit{Estrutura e Alimentação}} \\
        Fonte de Alimentação & 9V DC & 1 & 1 \\
        Placa Fenolite & 7x9 cm & 1 & 1 \\
        Amp. Operacional & LM324N (+ Soquete) & 1 & 1 \\
        LEDs & Verde e Vermelho & 1 de cada & 1 de cada \\
        \midrule
        \multicolumn{4}{l}{\textit{Resistores e Potenciômetros}} \\
        Resistor & 220 k$\Omega$ & 1 & 1 \\
        Resistor & 100 k$\Omega$ & 4 & 4 \\
        Resistor & 1 k$\Omega$ & 2 & 2 \\
        Resistor & 470 $\Omega$ & 1 & -- \\
        Resistor & 100 $\Omega$ & -- & 1 \\
        Potenciômetro & 10 k$\Omega$ Linear & 3 & 3\\
        \midrule
        \multicolumn{4}{l}{\textit{Componentes Específicos}} \\
        Chave Toggle & SPST (Liga/Desl.) & 3& -- \\
        Chave Bilateral & CD4066BE (+ Soquete) & -- & 1 \\
        \bottomrule
    \end{tabular}
\end{table}
\subsection{Implementação do Neurônio Artificial}

O circuito do neurônio foi baseado no trabalho apresentado em \cite{nutsvolts_perceptron}, com adaptações para atender aos requisitos deste projeto. O esquemático completo é apresentado na Fig. \ref{fig:neuronio_unico}, onde as etapas funcionais do processamento estão destacadas.

\hspace*{-3cm}
\begin{figure}[H] % 1. Começa a figura flutuante
    \centering     % 2. Centraliza
    
    % 3. O Resizebox envolve APENAS o desenho (circuitikz)
    \resizebox{1.0\textwidth}{!}{ 
        \iffalse \begin{circuitikz}

        

            % --- BLOCO 1: BUFFER DE ENTRADA (DIVISOR DE TENSÃO) ---

            \node[
                draw,
                dashed,
                thick,
                rounded corners,
                fit={(-33.5,1) (-29.2,-6.7)},
                label={[black]above:{\Large Divisor de tensão}}
            ] {};
            
            \draw (-30.2,0) -- (-29.8,0) node[red,above, midway]{$+9$V};
            
            \draw (-30,0) -- (-30,-1) 
                  to[R, a=$R_1$, l=$1\text{k}$] (-30,-3) 
                  -- (-30,-3.5) to[short, -*] (-30,-3.5) 
                  -- (-30,-4) 
                  to[R, a=$R_2$, l=$1\text{k}$] (-30,-6) 
                  node[tlground]{};

            \draw (-32,-4) node[op amp, rotate=180] (buffer) {};
            \draw (-30,-3.5) -- (buffer.+); 
            
            \draw (buffer.-) -- (-30.8,-5.5) -- (-33.2,-5.5) -- (buffer.out);

            \draw (buffer.-)   ++ (-0.15,0.25) node{10};
            \draw (buffer.+)   ++ (-0.15,0.25) node{9};
            \draw (buffer.out) ++ (0.2,0.2)    node{8};
            \draw (buffer.out) ++ (1.2,0)      node{C};

            \draw (-32,-5.5) -- (-32, -5.7);
            \draw (-32.2,-5.7) -- (-31.8, -5.7) node[blue, below, midway]{$+4.5$V};


            % --- BLOCO 2: ENTRADAS ---
            \node[
                draw,
                dashed,
                thick,
                rounded corners,
                fit={(-28.9,1) (-23.1,-6.7)},
                label={[black]above:{\Large Entradas}}
            ] {};
            
            % P1
            \draw (-28.2,0) -- (-27.8,0) node[red,above, midway]{$+9$V};
            \draw (-28,-0) to[potentiometer,n=Pot1, a=$10\text{k}$] (-28,-2) node[tlground]{};
            \draw (Pot1.wiper) ++ (0.1,0.3) node{$P_1$};

            \draw (Pot1.wiper) to[switch, a=$S_1$] (-25,-1)
                  to[R, a=$R_3$, l=$100\text{k}$] (-23,-1);

            % P2
            \draw (-28.2,-3) -- (-27.8,-3) node[red,above, midway]{$+9$V};
            \draw (-28,-3) to[potentiometer,n=Pot2, a=$10\text{k}$] (-28,-5) node[tlground]{};
            \draw (Pot2.wiper) ++ (0.1,0.3) node{$P_2$};

            \draw (Pot2.wiper) to[switch, a=$S_2$] (-25, -4) 
                  to[R, a=$R_4$, l=$100\text{k}$] (-23,-4);

            % --- BLOCO 3: SOMA (AMPOP A) ---

            \node[
                draw,
                dashed,
                thick,
                rounded corners,
                fit={(-22.8,1) (-17.5,-6.7)},
                label={[black]above:{\Large Amp. Não-Inversor}}
            ] {};

            \draw (-23,-1) -- (-22,-1) to[short,-*] (-22,-1);
            \draw (-23,-4) -- (-22,-4);
            
            \draw (-22,-1) ++ (0,+0.5) node{\large $V_{\text{in}}$};
                  
            \draw (-22,-1) -- (-22,-4) 
                  to[short,-*] (-22,-4) 
                  to[R, a=$R_5$, l=$100\text{k}$] (-22,-6) 
                  to[short,-*] (-22,-6) -- (-22, -6.2);
            \draw (-22.2,-6.2) -- (-21.8,-6.2) node[blue, below, midway]{$+4.5$V};

            

            \draw (-20,-1.5) node[op amp,yscale=-1](A){} (-22,-1) -- (A.+);

            \draw (A.-)   ++ (0.15,0.25) node{2};
            \draw (A.+)   ++ (0.15,0.25) node{3};
            \draw (A.out) ++ (-0.2,0.2)  node{1};
            \draw (A.out) ++ (-1.2,0)    node{A};

            \draw (A.-) -- (-21.2,-4) -- (-18.8,-4) 
                  to[short,-*] (-18.8,-4) 
                  to[R, a=$220\text{k}$, l=$R_6$] (A.out);
                  
            \draw (-18.8,-4) to[R, a=$R_7$, l=$100\text{k}$] (-18.8,-6) 
                  to[short,-*] (-18.8,-6) -- (-22,-6);


            % --- BLOCO 4: COMPARAÇÃO (AMPOP B) ---

            \node[
                draw,
                dashed,
                thick,
                rounded corners,
                fit={(-17.2,1) (-10.5, -4.4)},
                label={[black]above:{\Large Comparador}}
            ] {};
            
            \draw (-13.5,-2) node[op amp, yscale=-1](B){} (A.out) 
                  to[short,-*] (A.out) -- (B.+);
            
            \draw (B.-)    ++ (0.15,0.25) node{6};
            \draw (B.+)    ++ (0.15,0.25) node{5};
            \draw (B.out) ++ (-0.2,0.2)  node{7};
            \draw (B.out) ++ (-1.2,0)    node{B};
            \draw (B.out) ++ (0.4,-0.1) node{\large $V_b$};
            
            \draw (B.+)    ++ (-1.5,0.35) node{\large $V_{\text{a}}$};
            
            \draw (-16.2,-2.2) -- (-15.8,-2.2) node[red,above, midway]{$+9$V};
            \draw (-16, -2.2) to[potentiometer,n=Pot3, a=$10\text{k}$] (-16,-4.2) node[tlground]{};
            \draw (Pot3.wiper) ++ (0.1,0.3) node{$P_3$}; 
            \draw (Pot3.wiper) -- (-14.7,-3.2) -- (B.-);
            
            \draw (Pot3.wiper) ++ (0.9, -0.4) node{\large $V_{\text{bias}}$}; 
            
            \draw (B.out) to[R, a=$R_8$, l=$470$] (-12.4,-4) 
                  to[led, l=LED] (-12.4,-6) -- (-18.8,-6);
            
        \end{circuitikz} \fi
     } % 4. Fecha o resizebox aqui!

    % 5. Caption para o texto visível
    \caption{Diagrama esquemático de um único neurônio artificial.}
    
    % 6. Label para referência interna (opcional, mas recomendado)
    \label{fig:neuronio_unico}

\end{figure}

\section{Implementação do Neurônio Artificial}

A implementação do circuito baseia-se na operação matemática do Perceptron. A primeira etapa consiste na estabilização da tensão de referência. O \textbf{Bloco 1} apresenta um divisor de tensão acoplado a um Amplificador Operacional \textbf{C} na configuração de Seguidor de Tensão (\textit{Buffer}). Tal configuração é necessária devido à utilização de uma alimentação simples de $9$ V, o que impede a tensão de apresentar valores negativos reais. Para contornar essa limitação, é estabelecido uma tensão de referência em $+4.5$ V. Consequentemente, o potencial de $4.5$ V passa a ser o ponto zero lógico do sistema, permitindo que tensões inferiores a este valor representem as grandezas negativas necessárias para o cálculo dos pesos inibitórios\footnote{Pesos inibitórios são representados matematicamente por valores negativos ($w_i < 0$). Sua função é reduzir o valor da soma ponderada, contrapondo-se aos pesos excitatórios e dificultando a ativação do neurônio.}.

No \textbf{Bloco 2} da Fig. \ref{fig:neuronio_unico}, são estabelecidos os sinais de entrada do neurônio. Dois potenciômetros de $10$ k$\Omega$ são alimentados por $9$ V e referenciados ao GND ($0$ V), permitindo que a tensão no cursor varie entre esses dois valores conforme a relação,
\begin{equation}
    V_{pot} = 9\,\text{V} \cdot \left(\dfrac{R_{GND}}{10\,\text{k}\Omega}\right) 
\end{equation}

onde $R_{GND}$ é a resistência efetiva entre o cursor e o terra. A tensão ajustada em cada potenciômetro passa por uma chave em série com um resistor de $100$ k$\Omega$. Estas chaves definem as entradas binárias do neurônio: se a chave estiver aberta ($S_i = 0$), o circuito é interrompido e não há injeção de corrente, representando o valor lógico 0. Por outro lado, a chave fechada ($S_i = 1$) conecta a tensão do potenciômetro ao circuito, representando o valor lógico 1.

Assim, o sinal resultante no ponto de conexão dos resistores de $100$ k$\Omega$ será uma tensão $V_{\text{in}}$ equivalente à média das tensões de entrada somadas à tensão de referência. Isso ocorre pois a referência conecta-se ao mesmo nó através de uma resistência idêntica. Portanto:

\begin{equation}
V_{\text{in}} = \dfrac{4.5 + S_1 V_1 + S_2 V_2}{1 + S_1 + S_2}\label{eq:soma_ponderada}
\end{equation}

onde $V_i$ e $S_i$ representam, respectivamente, a tensão ajustada no potenciômetro e o estado da chave\footnote{Chave $i$ aberta $\rightarrow S_i = 0$; Chave $i$ fechada $\rightarrow S_i = 1$}.

A Eq. \ref{eq:soma_ponderada} evidencia que o circuito atua como um calculador de média aritmética ponderada e não como um somador puro. A variável binária $S_i$ atua simultaneamente no numerador (injeção de sinal) e no denominador (normalização).

Isso permite a simulação de sinapses inibitórias através do ajuste de $V_i$. Matematicamente, se inserirmos uma nova entrada $k$ cuja tensão $V_k$ seja menor que a tensão atual do nó ($V_{\text{in}}$), o novo valor de equilíbrio será reduzido. 

O \textbf{Bloco 3} ilustra o estágio de amplificação de soma. A tensão $V_{\text{in}}$ conecta-se à entrada não inversora do Amplificador Operacional \textbf{A}. Note que o resistor de terra da malha de realimentação ($R_7$) está conectado à tensão de referência ($4.5$ V) e não ao GND. Isso configura o circuito como um amplificador não inversor que atua sobre a \textit{variação} do sinal em relação à referência. Com um ganho definido por $G = 1 + \dfrac{R_6}{R_7} = 3.2$, a tensão de saída $V_\text{a}$ representa a referência somada ao desvio de entrada amplificado: 

\begin{equation}
V_\text{a} = 4.5 + G \cdot (V_{\text{in}} - 4.5)
\label{eq_va}
\end{equation}
Por fim, o \textbf{Bloco 4} descreve a etapa de decisão. A tensão resultante $V_{\text{a}}$ é aplicada à entrada não inversora do Amplificador Operacional \textbf{B}, operando como um comparador. Este sinal é confrontado com a tensão de referência $V_{\text{bias}}$, ajustada pelo terceiro potenciômetro, que atua como o limiar de ativação (ou peso $w_3$) do neurônio. Quando $V_{\text{a}}$ supera $V_{\text{bias}}$, a saída $V_{\text{b}}$ satura positivamente (próximo a $7.5$ V), acendendo o LED e indicando ativação. Caso contrário, a saída permanece próxima ao potencial de terra ($0$ V), mantendo o LED apagado. O comportamento binário da saída $y$ é descrito matematicamente pela função degrau:

\begin{equation} 
y = \begin{cases} 
1, & \text{se } V_{\text{a}} > V_{\text{bias}} \\ 
0, & \text{se } V_{\text{a}} \leq V_{\text{bias}} 
\end{cases} 
\label{eq:saida_binaria} 
\end{equation} 
A partir das equações descritas é possível compreender como que os pesos influenciam na decisão do neurônio. A fim de exemplificar tomemos pesos compatíveis com a porta NAND\footnote{ Tabela verdade da porta NAND: \\ 
\begin{tabular}{|c|c|c|} 
\hline $A$ & $B$ & $Y = \overline{A \cdot B}$ \\ 
\hline 0 & 0 & 1 \\ 
0 & 1 & 1 \\ 
1 & 0 & 1 \\ 
1 & 1 & 0 \\ 
\hline 
\end{tabular} } e verifiquemos cada etapa do circuito. O procedimento pode ser analisado na Tab. \ref{tab:resultados_nand}

\begin{table}[h!] 
\centering 
\caption{Resultados experimentais e calculados para a porta lógica NAND. Os pesos foram ajustados para $V_1 = 3.19$ V ($3.54$ k$\Omega$), $V_2 = 3.24$ V ($3.60$ k$\Omega$). e $V_3 = 2.06$ V ($2.29$ k$\Omega$).} \label{tab:resultados_nand} 
\begin{tabular}{cc|ccc|cc} 
\toprule 
\multicolumn{2}{c|}{\textbf{Entradas}} & \multicolumn{3}{c|}{\textbf{Tensões do Circuito}} & \multicolumn{2}{c}{\textbf{Resultado}} \\ $S_1$ & $S_2$ & $V_{\text{in}}$ (V) & $V_{\text{a}}$ (V) & $V_{\text{bias}}$ (V) & Saída ($y$) & Estado do LED \\ \midrule 0 & 0 & 4.50 & 4.50 & 2.06 & \textbf{1} & \textbf{LIGADO} \\ 0 & 1 & 3.87 & 2.48 & 2.06 & \textbf{1} & \textbf{LIGADO} \\ 1 & 0 & 3.85 & 2.40 & 2.06 & \textbf{1} & \textbf{LIGADO} \\ 1 & 1 & 3.64 & 1.76 & 2.06 & \textbf{0} & \textbf{DESLIGADO} \\ \bottomrule 
\end{tabular} 
\end{table} 

\subsection{Implementação do Backpropagation no Hardware}
\subsection{Expansão para Multicamadas}
\begin{figure}[H]
    \centering
    \hspace*{-3cm}
    \resizebox{1.3\textwidth}{!}{
        \iffalse \begin{circuitikz}

            % ==================================================================
            % NEURÔNIO 1 (TOPO ESQUERDA)
            % ==================================================================

            
            % Retângulo pontilhado
            \node[
                draw,
                dashed,
                thick,
                rounded corners,
                fit={(-33.5,1) (-18,-6)},
                label={[black]above:{\Large Neurônio 1}}
            ] {}; % <<< importante: fecha colchete na mesma linha da chave

                        
            % --- BUFFER DE ENTRADA N1 ---
            \draw (-30.2,0) -- (-29.8,0) node[red,above, midway]{$+9$V};
            \draw (-30,0) -- (-30,-1) 
                  to[R, a=$R_{{12}_1}$, l=$1\text{k}$] (-30,-3) 
                  -- (-30,-3.5) to[short, -*] (-30,-3.5) 
                  -- (-30,-4) 
                  to[R, a=$R_{{12}_2}$, l=$1\text{k}$] (-30,-6) 
                  node[tlground]{};

            \draw (-32.2,-4) node[op amp, rotate=180] (buf1) {};
            \draw (-30,-3.5) -- (buf1.+); 
            \draw (buf1.-) -- (-31,-5.5) -- (-33.4,-5.5) -- (buf1.out);

            % Numeração Buffer N1
            \draw (buf1.-)   ++ (-0.15,0.25) node{10};
            \draw (buf1.+)   ++ (-0.15,0.25) node{9};
            \draw (buf1.out) ++ (0.2,0.2)    node{8};
            \draw (buf1.out) ++ (1.2,0)      node{$C_{12}$};

            \draw (-32.2,-5.5) -- (-32.2, -5.7);
            \draw (-32.4,-5.7) -- (-32, -5.7) node[blue, below, midway]{$+4.5$V};

            % --- ENTRADAS N1 (Potenciômetros) ---
            % P1_1
            \draw (-28.2,0) -- (-27.8,0) node[red,above, midway]{$+9$V};
            \draw (-28,-0) to[potentiometer,n=Pot1_1, a=$10\text{k}$] (-28,-2) node[tlground]{};
            \draw (Pot1_1.wiper) ++ (0.1,0.3) node{$P_{1_1}$};

            % P1_2
            \draw (-28.2,-3) -- (-27.8,-3) node[red,above, midway]{$+9$V};
            \draw (-28,-3) to[potentiometer,n=Pot1_2, a=$10\text{k}$] (-28,-5) node[tlground]{};
            \draw (Pot1_2.wiper) ++ (0.1,0.3) node{$P_{1_2}$};

            % --- SOMADOR N1 (AmpOp A1) ---
            \draw (Pot1_1.wiper) to[switch, a=$S_{1_1}$] (-25, -1) 
                  to[R, a=$R_{1_3}$, l=$100\text{k}$] (-22,-1) 
                  to[short,-*] (-22,-1); 
                  
            \draw (-22,-1) -- (-22,-4) 
                  to[short,-*] (-22,-4) 
                  to[R, a=$R_{1_5}$, l=$100\text{k}$] (-22,-6) 
                  to[short,-*] (-22,-6) -- (-22, -6.2);
            \draw (-22.2,-6.2) -- (-21.8,-6.2) node[blue, below, midway]{$+4.5$V};

            \draw (Pot1_2.wiper) to[switch, a=$S_{1_2}$] (-25, -4) 
                  to[R, a=$R_{1_4}$, l=$100\text{k}$] (-22,-4);

            \draw (-20,-1.5) node[op amp,yscale=-1](A1){} (-22,-1) -- (A1.+);

            % Numeração A1
            \draw (A1.-)   ++ (0.15,0.25) node{2};
            \draw (A1.+)   ++ (0.15,0.25) node{3};
            \draw (A1.out) ++ (-0.2,0.2)  node{1};
            \draw (A1.out) ++ (-1.2,0)    node{$A_1$};

            \draw (A1.-) -- (-21.2,-4) -- (-18.8,-4) 
                  to[short,-*] (-18.8,-4) 
                  to[R, a=$220\text{k}$, l=$R_{1_6}$] (A1.out);
                  
            \draw (-18.8,-4) to[R, a=$R_{1_7}$, l=$100\text{k}$] (-18.8,-6) 
                   -- (-22,-6);

            % --- COMPARADOR N1 (AmpOp B1) ---
            \draw (-14.5,-2) node[op amp, yscale=-1](B1){} (A1.out) 
                  to[short,-*] (A1.out) -- (B1.+);

            % Numeração B1
            \draw (B1.-)   ++ (0.15,0.25) node{6};
            \draw (B1.+)   ++ (0.15,0.25) node{5};
            \draw (B1.out) ++ (-0.2,0.2)  node{7};
            \draw (B1.out) ++ (-1.2,0)    node{$B_1$};

            % Potenciômetro Threshold P1_3
            \draw (-17.2,-2.2) -- (-16.8,-2.2) node[red,above, midway]{$+9$V};
            \draw (-17, -2.2) to[potentiometer,n=Pot1_3, a=$10\text{k}$] (-17,-4.2) node[tlground]{};
            \draw (Pot1_3.wiper) ++ (0.1,0.3) node{$P_{1_3}$}; 
            \draw (Pot1_3.wiper) -- (-15.7,-3.2) -- (B1.-);


            % ==================================================================
            % NEURÔNIO 2 (BAIXO ESQUERDA - Deslocado Y-7)
            % ==================================================================
            \node[
                draw,
                dashed,
                thick,
                rounded corners,
                fit={(-28.9,-7) (-18,-14)},
                label={[black]below:{\Large Neurônio 2}}
            ] {}; % <<< importante: fecha colchete na mesma linha da chave
    
            % --- ENTRADAS N2 ---
            % P2_1
            \draw (-28.2,-7) -- (-27.8,-7) node[red,above, midway]{$+9$V};
            \draw (-28,-7) to[potentiometer,n=Pot2_1, a=$10\text{k}$] (-28,-9) node[tlground]{};
            \draw (Pot2_1.wiper) ++ (0.1,0.3) node{$P_{2_1}$};

            % P2_2
            \draw (-28.2,-10) -- (-27.8,-10) node[red,above, midway]{$+9$V};
            \draw (-28,-10) to[potentiometer,n=Pot2_2, a=$10\text{k}$] (-28,-12) node[tlground]{};
            \draw (Pot2_2.wiper) ++ (0.1,0.3) node{$P_{2_2}$};

            % --- SOMADOR N2 (AmpOp A2) ---
            \draw (Pot2_1.wiper) to[switch, a=$S_{2_1}$] (-25, -8) 
                  to[R, a=$R_{2_3}$, l=$100\text{k}$] (-22,-8) 
                  to[short,-*] (-22,-8); 
                  
            \draw (-22,-8) -- (-22,-11) 
                  to[short,-*] (-22,-11) 
                  to[R, a=$R_{2_5}$, l=$100\text{k}$] (-22,-13) 
                  to[short,-*] (-22,-13) -- (-22, -13.2);
            \draw (-22.2,-13.2) -- (-21.8,-13.2) node[blue, below, midway]{$+4.5$V};

            \draw (Pot2_2.wiper) to[switch, a=$S_{2_2}$] (-25, -11) 
                  to[R, a=$R_{2_4}$, l=$100\text{k}$] (-22,-11);

            \draw (-20,-8.5) node[op amp,yscale=-1](A2){} (-22,-8) -- (A2.+);

            % Numeração A2
            \draw (A2.-)   ++ (0.15,0.25) node{2};
            \draw (A2.+)   ++ (0.15,0.25) node{3};
            \draw (A2.out) ++ (-0.2,0.2)  node{1};
            \draw (A2.out) ++ (-1.2,0)    node{$A_2$};

            \draw (A2.-) -- (-21.2,-11) -- (-18.8,-11) 
                  to[short,-*] (-18.8,-11) 
                  to[R, a=$220\text{k}$, l=$R_{2_6}$] (A2.out);
                  
            \draw (-18.8,-11) to[R, a=$R_{2_7}$, l=$100\text{k}$] (-18.8,-13) 
                   -- (-22,-13);

            % --- COMPARADOR N2 (AmpOp B2) ---
            \draw (-14.5,-9) node[op amp, yscale=-1](B2){} (A2.out) 
                  to[short,-*] (A2.out) -- (B2.+);

            % Numeração B2
            \draw (B2.-)   ++ (0.15,0.25) node{6};
            \draw (B2.+)   ++ (0.15,0.25) node{5};
            \draw (B2.out) ++ (-0.2,0.2)  node{7};
            \draw (B2.out) ++ (-1.2,0)    node{$B_2$};

            % Potenciômetro Threshold P2_3
            \draw (-17.2,-9.2) -- (-16.8,-9.2) node[red,above, midway]{$+9$V};
            \draw (-17, -9.2) to[potentiometer,n=Pot2_3, a=$10\text{k}$] (-17,-11.2) node[tlground]{};
            \draw (Pot2_3.wiper) ++ (0.1,0.3) node{$P_{2_3}$}; 
            \draw (Pot2_3.wiper) -- (-15.7,-10.2) -- (B2.-);


            % ==================================================================
            % NEURÔNIO 3 (DIREITA - Deslocado X+14.8, Y-3.5)
            % ==================================================================

            \node[
                draw,
                dashed,
                thick,
                rounded corners,
                fit={(-33.5,-7) (-29.2,-14)},
                label={[black]below:{\Large Neurônio 3}}
            ] {}; %caixa do neuronio 3
            
            \node[
                draw,
                dashed,
                thick,
                rounded corners,
                fit={(-17.3,-1) (3,-11.5)},
                label={[black]above:{\Large Neurônio 3}}
            ] {}; %caixa do neuronio 3

            

            % --- BUFFER DE ENTRADA N3 ---
            \draw (-30.2,-7) -- (-29.8,-7) node[orange ,above, midway]{$+7.5$V};
            \draw (-30,-7) -- (-30,-8) 
                  to[R, a=$R_{3_1}$, l=$1\text{k}$] (-30,-10) 
                  -- (-30,-10.5) to[short, -*] (-30,-10.5) 
                  -- (-30,-11) 
                  to[R, a=$R_{3_2}$, l=$1\text{k}$] (-30,-13) 
                  node[tlground]{};

            \draw (-32.2,-11) node[op amp, rotate=180] (buf2) {};
            \draw (-30,-10.5) -- (buf2.+); 
            \draw (buf2.-) -- (-31,-12.5) -- (-33.4,-12.5) -- (buf2.out);

            % Numeração Buffer N2
            \draw (buf2.-)   ++ (-0.15,0.25) node{10};
            \draw (buf2.+)   ++ (-0.15,0.25) node{9};
            \draw (buf2.out) ++ (0.2,0.2)    node{8};
            \draw (buf2.out) ++ (1.2,0)      node{$C_3$};

            \draw (-32.2,-12.5) -- (-32.2, -12.7);
            \draw (-32.4,-12.7) -- (-32, -12.7) node[purple, below, midway]{$+3.75$V};

            % --- ENTRADAS N3 ---
            % P3_1
            \draw (B1.out) -- (-13.2,-2) -- (-13.2,-3.5) to[potentiometer,n=Pot3_1, a=$10\text{k}$] (-13.2,-5.5) node[tlground]{};
            \draw (Pot3_1.wiper) ++ (0.1,0.3) node{$P_{3_1}$};

            % P3_2
            \draw (-13.2,-6.5) node[tlground, rotate=180]{};
            \draw (-13.2,-6.5) to[potentiometer,n=Pot3_2, a=$10\text{k}$] (-13.2,-8.5) -- (-13.2,-9) -- (B2.out);
            \draw (Pot3_2.wiper) ++ (0.1,0.3) node{$P_{3_2}$};

            % --- SOMADOR N3 (AmpOp A3) ---
            \draw (Pot3_1.wiper) to[switch, a=$S_{3_1}$] (-10.2, -4.5) 
                  to[R, a=$R_{3_3}$, l=$100\text{k}$] (-7.2,-4.5) 
                  to[short,-*] (-7.2,-4.5); 
                  
            \draw (-7.2,-4.5) -- (-7.2,-7.5) 
                  to[short,-*] (-7.2,-7.5) 
                  to[R, a=$R_{3_5}$, l=$100\text{k}$] (-7.2,-9.5) 
                  to[short,-*] (-7.2,-9.5) -- (-7.2, -9.7);
            \draw (-7.4,-9.7) -- (-7,-9.7) node[purple, below, midway]{$+3.75$V};

            \draw (Pot3_2.wiper) to[switch, a=$S_{3_2}$] (-10.2, -7.5) 
                  to[R, a=$R_{3_4}$, l=$100\text{k}$] (-7.2,-7.5);

            \draw (-5.2,-5) node[op amp,yscale=-1](A3){} (-7.2,-4.5) -- (A3.+);

            % Numeração A3
            \draw (A3.-)   ++ (0.15,0.25) node{2};
            \draw (A3.+)   ++ (0.15,0.25) node{3};
            \draw (A3.out) ++ (-0.2,0.2)  node{1};
            \draw (A3.out) ++ (-1.2,0)    node{$A_3$};

            \draw (A3.-) -- (-6.4,-7.5) -- (-4,-7.5) 
                  to[short,-*] (-4,-7.5) 
                  to[R, a=$220\text{k}$, l=$R_{3_6}$] (A3.out);
                  
            \draw (-4,-7.5) to[R, a=$R_{3_7}$, l=$100\text{k}$] (-4,-9.5) to[short,-*] (-4,-9.5) -- (-7.2,-9.5);

            % --- COMPARADOR N3 (AmpOp B3) ---
            \draw (0.3,-5.5) node[op amp, yscale=-1](B3){} (A3.out) 
                  to[short,-*] (A3.out) -- (B3.+);

            % Numeração B3
            \draw (B3.-)   ++ (0.15,0.25) node{6};
            \draw (B3.+)   ++ (0.15,0.25) node{5};
            \draw (B3.out) ++ (-0.2,0.2)  node{7};
            \draw (B3.out) ++ (-1.2,0)    node{$B_3$};

            % Potenciômetro Threshold P3_3
            \draw (-2.4,-5.7) -- (-2,-5.7) node[orange,above, midway]{$+7.5$V};
            \draw (-2.2, -5.7) to[potentiometer,n=Pot3_3, a=$10\text{k}$] (-2.2,-7.7) node[tlground]{};
            \draw (Pot3_3.wiper) ++ (0.1,0.3) node{$P_{3_3}$}; 
            
            \draw (Pot3_3.wiper) -- (-0.9,-6.7) -- (B3.-);

            \draw (B3.out) to[R, a=$R_8$, l=$100$] (1.5,-7.5) 
            to[led, l=LED] (1.5,-9.5) -- (-4,-9.5);

        \end{circuitikz} \fi
    }
    \caption{Circuito completo da rede neural artificial composto por três neurônios.}
    \label{fig:rede_xor}
\end{figure}












\section{Resultados e Discussão}

\subsection{Scripts para cálculo de pesos}

Foram construídos scripts em Python que fazem o treinamento por backpropagation para o circuito com um e três neurônios. O script pede a saída desejada (0 ou 1) para cada um dos inputs ($00, 01, 10, 11$) e retorna com os pesos necessários para criar a porta desejada. A Fig. \ref{fig:scriptXOR} apresenta um exemplo de resposta do script para o circuito de três neurônios com a porta XOR.


\begin{figure}[H]
    \centering
    % \includegraphics[width=0.5\linewidth]{Imagens/PesosScriptXOR3N.png}
    \caption{Exemplo de resposta do script para circuito de três neurônios e a chave XOR}
    \label{fig:scriptXOR}
\end{figure}

Note que o script usa as equações do circuito para validar a eficácia dos pesos e verificar se as saídas para cada combinação de inputs estão corretas. 

Esses script estão disponíveis em: POR REFERENCIA AQUI AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAaaAAAAAAAAAAAAAAAAAAAAAA e foram utilizados para obter os pesos da grande maioria dos testes desse projeto.

\subsection{Circuito com um neurônio}

Em relação a montagem dos circuitos analógicos, o projeto teve como resultado a construção de 3 neurônios funcionais independentes do tipo perceptron com dois inputs, segue ilustrado na Fig.\ref{fig:1neuronio} um desses circuitos.

\begin{figure}[H]
    \centering
    % \includegraphics[width=0.5\linewidth]{Imagens/1neuronio.png}
    \caption{Imagem de um dos neurônios analógicos construídos. Em azul, a chave do input A; em vermelho, a chave do input B; em verde, o output do sistema (LED); e em roxo, os potenciômetros de cada peso.}
    \label{fig:1neuronio}
\end{figure}

Para cada neurônio, o peso de cada input e do bias são dados pelos três potenciômetros na lateral; o output é dado pela cor do LED, onde verde significa 1 e vermelho significa 0; e os inputs são dados pelo fechamento de chaves ou, para o neurônio na camada de saída, por fios conectados na saída dos outros neurônios. Para regular os pesos, é medido a tensão no pino central de cada potenciômetro.

Além disso, existe uma chave para ligar e desligar a fonte do neurônio para fins de segurança. Os neurônios precisam ser alimentados com uma fonte de 9V, com exceção do neurônio da camada de saída que precisa ser alimentado com 7,5V por conta da saturação máxima dos amplificadores operacionais dos neurônios de entrada.

Com esse neurônio, é possível gerar quaisquer portas lógicas, com exceção de XOR e XNOR, que não são linearmente separáveis. A Fig. \ref{fig:portaNAND} apresenta um exemplo de neurônio ajustado para porta lógica NAND. Note que a saída é apenas 0 (LED vermelho) quando ambas as entradas são 1 (chave fechada).

\begin{figure}[H]
    \centering
    % --- Primeira Imagem (a) ---
    \begin{minipage}[b]{0.45\textwidth}
        \centering
        % \includegraphics[width=\textwidth]{Imagens/neuronio nand 00.jpeg}
        \centerline{Input 00}
    \end{minipage}
    \hfill 
    \begin{minipage}[b]{0.45\textwidth}
        \centering
        % \includegraphics[ width=\textwidth]{Imagens/neuronio nand 01.jpeg}
        \centerline{Input 01}
    \end{minipage}
    
    \vspace{0.5cm} 
    
    % --- Terceira Imagem (c) ---
    \begin{minipage}[b]{0.45\textwidth}
        \centering
        % \includegraphics[width=\textwidth]{Imagens/neuronio nand 10.jpeg}
        \centerline{Input 10}
    \end{minipage}
    \hfill
    % --- Quarta Imagem (d) ---
    \begin{minipage}[b]{0.45\textwidth}
        \centering
        % \includegraphics[width=\textwidth]{Imagens/neuronio nand 11.jpeg}
        \centerline{Input 11}
    \end{minipage}
    
    \caption{Neurônio com pesos ajustados para reproduzir porta NAND.}
    \label{fig:portaNAND}
\end{figure}


\subsection{Integração entre os três neurônios}

Além disso, foi realizada com sucesso a integração desses três neurônios de forma a criar uma pequena rede neural com dois neurônios na camada de entrada conectados a um neurônio na camada de saída, como ilustrado na Fig. \ref{fig:circuito3neuronios}

\begin{figure}[H]
    \centering
    % \includegraphics[width = 1\linewidth]{Imagens/circuito3neuronios.png}
    \caption{Imagem de circuito de rede neural construído. Indicado em azul, a chave do input A; em vermelho, a chave do input B; em amarelo, o output do sistema (LED); e em laranja, o output dos neurônios de entrada.}
    \label{fig:circuito3neuronios}
\end{figure}

Um ponto importante a se ressaltar é que, como cada neurônio de entrada funciona de forma independente, as chaves de entrada também são independentes. Para resolver isso, tomamos que a mesma chave de entrada para ambos neurônios da camada de entrada tem que ser iguais, gerando assim inputs iguais. Ou seja, caso a chave A de um neurônio esteja aberta, a chave A do outro tem que estar aberta, e vice-versa. Isso é apenas uma simplificação experimental para esse protótipo do circuito. 

Dessa forma, temos apenas dois inputs (cada par de chaves) e um output (LED do neurônio da camada de saída). Note que os LEDs dos neurônios da camada de entrada são os outputs desses neurônios e não tem um significado físico.

Como dito anteriormente, as portas XOR (ou exclusivo) e XNOR não são linearmente separáveis e não podem ser feitas por apenas um neurônio, de forma que esse circuito com três neurônios integrados é necessário para reproduzir o comportamento dessas portas. A Fig. \ref{fig:portaXOR} apresenta exemplo de rede neural ajustada para porta lógica XOR. Note que a saída é 1 (LED verde) quando qualquer uma das entradas é 1, mas não ambas.

\begin{figure}[H]
    \centering
    % --- Primeira Imagem (a) ---
    \begin{minipage}[b]{0.45\textwidth}
        \centering
        % \includegraphics[width=\textwidth]{Imagens/neuronios XOR 00.jpeg}
        \centerline{Input 00}
    \end{minipage}
    \hfill 
    \begin{minipage}[b]{0.45\textwidth}
        \centering
        % \includegraphics[ width=\textwidth]{Imagens/neuronios XOR 01.jpeg}
        \centerline{Input 01}
    \end{minipage}
    
    \vspace{0.5cm} 
    
    % --- Terceira Imagem (c) ---
    \begin{minipage}[b]{0.45\textwidth}
        \centering
        % \includegraphics[width=\textwidth]{Imagens/neuronios XOR 10.jpeg}
        \centerline{Input 10}
    \end{minipage}
    \hfill
    % --- Quarta Imagem (d) ---
    \begin{minipage}[b]{0.45\textwidth}
        \centering
        % \includegraphics[width=\textwidth]{Imagens/neuronios XOR 11.jpeg}
        \centerline{Input 11}
    \end{minipage}
    
    \caption{Rede neural com pesos ajustados para reproduzir porta XOR.}
    \label{fig:portaXOR}
\end{figure}

\subsection{Discussões}

Uma das maiores dificuldades desse projeto foi a análise do circuito original e fazer as adaptações necessárias, de forma que foram necessários diversos testes em simulações para obter esse circuito final.

Durante a montagem física, o mal contato nas conexões da protoboard e de alguns componentes soldados, asim como eventuais componentes queimados exigiram um trabalho constate de investigação e correção do circuito, demandando tempo significativo da construção dos neurônios. 

O método de ajuste dos pesos via potenciômetros se mostrou simples e eficaz. Observou-se uma boa estabilidade dos pesos, com uma variação de por volta de 0,03V, o circuito continuava com um comportamento lógico estável. Entretanto, ocorreram alguns casos que os pesos calculados pelo script eram validados na simulação, mas falhavam no circuito físico. Isso provavelmente se deve às idealizações físicas das simulações, que não consideram perfeitamente efeitos de componentes analógicos. Nessas situações, a solução era ajuste manualmente o peso no bias ou gerar novos conjuntos de pesos pelo script até encontrar um conjunto funcional.

\section{Conclusão}

O projeto atingiu seu objetivo principal de construir uma rede neural analógica com neurônios do tipo perceptron. Verificou-se que é possível traduzir a lógica dos neurônios, como as somas ponderadas e funções de ativação com componentes eletrônicos.

A metodologia híbrida adotada se mostrou relativamente eficaz. A utilização de scripts em Python para o treinamento por backpropagation junto com a alteração manual dos pesos através de potenciômetros foi uma boa alternativa a circuitos de auto-aprendizagem.

Além disso, os testes realizados confirmaram a robustez do sistema. Foi possível reproduzir diversas portas lógicas, incluindo as linearmente separáveis (AND, OR, NAND, NOR, etc) e, através da integração de três neurônios, as portas não lineares (XOR, XNOR).

Por fim, o experimento evidenciou as limitações da computação analógica. Notou-se que lidar com componentes analógicos demanda um trabalho muito maior a suas contrapartes digitais. Além disso, a escalabilidade para redes neurais maiores enfrentaria desafios significativos, de forma a ser inviável, mas esse projeto verificou-se que seria possível faze-lá.

% --- Referências ---
% \bibliographystyle{abntex2-alf}
% \bibliography{bibliografia}      
\end{document}